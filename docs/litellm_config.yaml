model_list:
  - model_name: o3
    litellm_params:
      model: openai/o3
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      is_private: true
      max_output_tokens: 32768
      tokens_per_minute: 8000000

  - model_name: o4-mini
    litellm_params:
      model: openai/o4-mini
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      is_private: true
      max_output_tokens: 32768
      tokens_per_minute: 40000000

  - model_name: gpt-4.1
    litellm_params:
      model: openai/gpt-4.1
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      is_private: true
      tokens_per_minute: 8000000

  - model_name: gemini-2.5-pro
    litellm_params:
      model: gemini/gemini-2.5-pro
      api_key: "os.environ/GEMINI_API_KEY"
    model_info:
      is_private: true
      tokens_per_minute: 2000000

  - model_name: gemini-2.5-flash
    litellm_params:
      model: gemini/gemini-2.5-flash
      api_key: "os.environ/GEMINI_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      tokens_per_minute: 2000000

  - model_name: gemini-2.5-flash-lite
    litellm_params:
      model: gemini/gemini-2.5-flash-lite-preview-06-17
      api_key: "os.environ/GEMINI_API_KEY"
    model_info:
      supports_reasoning_disable: true
      free_tier_eligible: true
      is_private: true
      tokens_per_minute: 8000000

  - model_name: gemini-2.0-flash
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: "os.environ/GEMINI_API_KEY"
    model_info:
      free_tier_eligible: true
      is_private: true
      tokens_per_minute: 8000000

  - model_name: gemini-2.0-flash-lite
    litellm_params:
      model: gemini/gemini-2.0-flash-lite
      api_key: "os.environ/GEMINI_API_KEY"
    model_info:
      free_tier_eligible: true
      is_private: true
      tokens_per_minute: 8000000

  - model_name: claude-4-opus
    litellm_params:
      model: anthropic/claude-opus-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      # leave max_output_tokens alone, unlike sonnet 4, opus is capped at 32000 (not 32k)
      max_concurrent_requests: 1

  - model_name: claude-4-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-20250514
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      # Anthropic errors out if input + max_output > 200k. Default max_output is 128k,
      # which is weighted the wrong way for typical coding tasks
      max_output_tokens: 32768
      max_concurrent_requests: 1

  - model_name: claude-4.5-sonnet
    litellm_params:
      model: anthropic/claude-sonnet-4-5-20250929
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 64000
      input_cost_per_token: 1e-06
      output_cost_per_token: 5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: claude-haiku-4.5
    litellm_params:
      model: anthropic/claude-haiku-4-5-20251001
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 64000
      input_cost_per_token: 1e-06
      output_cost_per_token: 5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: deepseek-R1
    litellm_params:
      model: deepseek/deepseek-reasoner
      api_key: "os.environ/DEEPSEEK_API_KEY"
    model_info:
      is_private: false
      max_concurrent_requests: 100

  - model_name: deepseek-v3.1
    litellm_params:
      model: deepseek/deepseek-chat
      api_key: "os.environ/DEEPSEEK_API_KEY"
    model_info:
      max_input_tokens: 128000
      free_tier_eligible: true
      is_private: false
      max_concurrent_requests: 100

  - model_name: grok-3-beta
    litellm_params:
      model: xai/grok-3-beta
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      is_private: true
      max_concurrent_requests: 10

  - model_name: grok-3-fast-beta
    litellm_params:
      model: xai/grok-3-fast-beta
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      is_private: true
      max_concurrent_requests: 10

  - model_name: grok-3-mini-beta
    litellm_params:
      model: xai/grok-3-mini-beta
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      is_private: true
      max_concurrent_requests: 8

  - model_name: qwen3-coder
    litellm_params:
      model: openrouter/qwen/qwen3-coder
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 262144
      max_input_tokens: 262144
      max_output_tokens: 32768
      input_cost_per_token: 1e-06
      output_cost_per_token: 5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: qwen3-max
    litellm_params:
      model: openrouter/qwen/qwen3-max
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 262144
      max_input_tokens: 262144
      max_output_tokens: 32768
      input_cost_per_token: 1e-06
      output_cost_per_token: 5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: qwen3-coder-30b
    litellm_params:
      model: openrouter/qwen/qwen3-coder-30b-a3b-instruct
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 200000
      max_input_tokens: 200000
      max_output_tokens: 32768
      input_cost_per_token: 1e-06
      output_cost_per_token: 5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: qwen3-next
    litellm_params:
      model: openrouter/qwen/qwen3-next-80b-a3b-instruct
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 131000
      max_input_tokens: 131000
      max_output_tokens: 32768
      input_cost_per_token: 5e-06
      output_cost_per_token: 2e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: glm-4.6
    litellm_params:
      model: openrouter/z-ai/glm-4.6
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 131000
      max_input_tokens: 131000
      max_output_tokens: 32768
      input_cost_per_token: 5e-06
      output_cost_per_token: 2e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: glm-4.5-air
    litellm_params:
      model: openrouter/z-ai/glm-4.5-air
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 131000
      max_input_tokens: 131000
      max_output_tokens: 32768
      input_cost_per_token: 5e-06
      output_cost_per_token: 2e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: claude-4-1-opus
    litellm_params:
      model: anthropic/claude-opus-4-1-20250805
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      max_concurrent_requests: 1
      cache_creation_input_token_cost: 1.875e-05
      cache_read_input_token_cost: 1.5e-06
      litellm_provider: "anthropic"
      mode: "chat"
      supports_function_calling: true
      supports_vision: true
      tool_use_system_prompt_tokens: 159
      supports_assistant_prefill: true
      supports_pdf_input: true
      supports_prompt_caching: true
      supports_response_schema: true
      supports_tool_choice: true
      supports_reasoning: true
      supports_computer_use: true

  - model_name: claude-4-1-opus
    litellm_params:
      model: anthropic/claude-opus-4-1-20250805
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      max_concurrent_requests: 1
      cache_creation_input_token_cost: 1.875e-05
      cache_read_input_token_cost: 1.5e-06
      litellm_provider: "anthropic"
      mode: "chat"
      supports_function_calling: true
      supports_vision: true
      tool_use_system_prompt_tokens: 159
      supports_assistant_prefill: true
      supports_pdf_input: true
      supports_prompt_caching: true
      supports_response_schema: true
      supports_tool_choice: true
      supports_reasoning: true
      supports_computer_use: true

  - model_name: claude-4-5-opus
    litellm_params:
      model: anthropic/claude-opus-4-5-20251101
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      supports_reasoning_disable: true
      is_private: true
      max_concurrent_requests: 1
      cache_creation_input_token_cost: 1.875e-05
      cache_read_input_token_cost: 1.5e-06
      litellm_provider: "anthropic"
      mode: "chat"
      supports_function_calling: true
      supports_vision: true
      tool_use_system_prompt_tokens: 159
      supports_assistant_prefill: true
      supports_pdf_input: true
      supports_prompt_caching: true
      supports_response_schema: true
      supports_tool_choice: true
      supports_reasoning: true
      supports_computer_use: true

  - model_name: gpt-oss-120b
    litellm_params:
      model: openrouter/openai/gpt-oss-120b
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 131072
      max_input_tokens: 131072
      max_output_tokens: 32766
      input_cost_per_token: 1.5e-07
      output_cost_per_token: 6e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      supported_openai_params: ["max_completion_tokens","stream", "stream_options","temperature","tools","tool_choice","function_call","functions","max_retries","extra_headers","response_format"]
      tokens_per_minute: 8000000

  - model_name: gpt-5
    litellm_params:
      model: openai/gpt-5
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 400000
      max_input_tokens: 400000
      max_output_tokens: 32766
      input_cost_per_token: 1.25e-06
      output_cost_per_token: 1e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      supported_openai_params: ["max_completion_tokens","stream", "stream_options","temperature","tools","tool_choice","function_call","functions","max_retries","extra_headers","response_format"]
      tokens_per_minute: 8000000

  - model_name: gpt-5-codex
    litellm_params:
      model: openai/gpt-5-codex
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 400000
      max_input_tokens: 400000
      max_output_tokens: 32766
      input_cost_per_token: 1.25e-06
      output_cost_per_token: 1e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      supported_openai_params: ["max_completion_tokens","stream", "stream_options","temperature","tools","tool_choice","function_call","functions","max_retries","extra_headers","response_format"]
      tokens_per_minute: 8000000

  - model_name: gpt-5-mini
    litellm_params:
      model: openai/gpt-5-mini
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 400000
      max_input_tokens: 400000
      max_output_tokens: 32766
      input_cost_per_token: 2.5e-07
      output_cost_per_token: 2e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      supported_openai_params: ["max_completion_tokens","stream", "stream_options","temperature","tools","tool_choice","function_call","functions","max_retries","extra_headers","response_format"]
      tokens_per_minute: 8000000

  - model_name: gpt-5-nano
    litellm_params:
      model: openai/gpt-5-nano
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 400000
      max_input_tokens: 400000
      max_output_tokens: 32766
      input_cost_per_token: 5e-08
      output_cost_per_token: 4e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      supported_openai_params: ["max_completion_tokens","stream", "stream_options","temperature","tools","tool_choice","function_call","functions","max_retries","extra_headers","response_format"]
      tokens_per_minute: 8000000

  - model_name: groq-kimi-k2
    litellm_params:
      model: groq/moonshotai/kimi-k2-instruct
      api_key: "os.environ/GROQ_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 131072
      max_input_tokens: 131072
      max_output_tokens: 16384
      input_cost_per_token: 1e-06
      output_cost_per_token: 3e-06
      litellm_provider: groq
      mode: chat
      supports_function_calling: true
      supports_response_schema: true
      supports_tool_choice: true
      max_concurrent_requests: 1

  - model_name: gpt-4o-transcribe
    litellm_params:
      model: openai/gpt-4o-transcribe
      api_key: "os.environ/OPENAI_API_KEY"
    model_info:
      free_tier_eligible: true
      is_private: true

  - model_name: gpt-oss-120b
    litellm_params:
      model: openrouter/openai/gpt-oss-120b
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: true
      is_private: true
      max_tokens: 131072
      max_input_tokens: 131072
      max_output_tokens: 32766
      input_cost_per_token: 1.5e-07
      output_cost_per_token: 6.0e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: gpt-oss-20b
    litellm_params:
      model: openrouter/openai/gpt-oss-20b
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: true
      is_private: true
      max_tokens: 131072
      max_input_tokens: 131072
      max_output_tokens: 32766
      input_cost_per_token: 1.5e-07
      output_cost_per_token: 6.0e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: true
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: grok-code-fast-1
    litellm_params:
      model: xai/grok-code-fast-1
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 256000
      max_input_tokens: 256000
      max_output_tokens: 256000 # ???
      input_cost_per_token: 2e-07
      output_cost_per_token: 1.5e-06
      mode: chat
      supports_tool_choice: true
      uses_think_tags: false
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 8000000

  - model_name: grok-4-fast
    litellm_params:
      model: xai/grok-4-fast-reasoning
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 2000000
      max_input_tokens: 2000000
      max_output_tokens: 256000 # ???
      input_cost_per_token: 2e-07
      output_cost_per_token: 5e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: false
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 4000000

  - model_name: grok-4.1-fast
    litellm_params:
      model: xai/grok-4-1-fast-reasoning
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 2000000
      max_input_tokens: 2000000
      max_output_tokens: 256000 # ???
      input_cost_per_token: 2e-07
      output_cost_per_token: 5e-07
      mode: chat
      supports_tool_choice: true
      uses_think_tags: false
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 4000000

  - model_name: grok-4
    litellm_params:
      model: xai/grok-4-0709
      api_key: "os.environ/GROK_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 2000000
      max_input_tokens: 256000
      max_output_tokens: 64000 # ???
      input_cost_per_token: 3e-06
      output_cost_per_token: 1.5e-05
      mode: chat
      supports_tool_choice: true
      uses_think_tags: false
      supports_reasoning: true
      supports_function_calling: true
      tokens_per_minute: 2000000

  - model_name: kimi-k2-thinking
    litellm_params:
      model: openrouter/moonshotai/kimi-k2-thinking
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 262000
      max_input_tokens: 262000
      max_output_tokens: 64000
      input_cost_per_token: 1e-06
      output_cost_per_token: 3e-06
      mode: chat
      supports_function_calling: true
      supports_response_schema: true
      supports_tool_choice: true
      max_concurrent_requests: 1

  - model_name: m2
    litellm_params:
      model: openrouter/minimax/minimax-m2
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      free_tier_eligible: false
      is_private: true
      max_tokens: 204000
      max_input_tokens: 204000
      max_output_tokens: 131000
      input_cost_per_token: 1e-06
      output_cost_per_token: 3e-06
      mode: chat
      supports_function_calling: true
      supports_response_schema: true
      supports_tool_choice: true
      max_concurrent_requests: 1

litellm_settings:
  num_retries: 0
